{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variable Configuration\n",
    "\n",
    "To protect the security of API keys, we need to set them as environment variables. Environment variables allow us to access them without writing sensitive information into the code.\n",
    "\n",
    "You can set environment variables in one of two ways:\n",
    "1. Set environment variables in the terminal.\n",
    "2. Set environment variables in the Python script.\n",
    "\n",
    "Here we will set them directly in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T08:44:19.771087Z",
     "iopub.status.busy": "2024-12-21T08:44:19.770708Z",
     "iopub.status.idle": "2024-12-21T08:44:19.797895Z",
     "shell.execute_reply": "2024-12-21T08:44:19.796831Z",
     "shell.execute_reply.started": "2024-12-21T08:44:19.771044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set DASHSCOPE_API_KEY to your Qwen API key\n",
    "os.environ['OPENAI_API_KEY'] = 'Your-Qwen-API-Key'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取环境变量\n",
    "\n",
    "使用 `os.getenv()` 函数来获取环境变量的值，这样我们可以在代码中安全地访问API密钥。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T08:44:45.157557Z",
     "iopub.status.busy": "2024-12-21T08:44:45.156870Z",
     "iopub.status.idle": "2024-12-21T08:44:45.163154Z",
     "shell.execute_reply": "2024-12-21T08:44:45.161960Z",
     "shell.execute_reply.started": "2024-12-21T08:44:45.157520Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your-Qwen-API-Key\n"
     ]
    }
   ],
   "source": [
    "# Get API key\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Print the key to confirm it has been successfully read\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Libraries\n",
    "\n",
    "Next, we need to install the openai library to interact with Alibaba Cloud's large model API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T08:45:10.867388Z",
     "iopub.status.busy": "2024-12-21T08:45:10.867013Z",
     "iopub.status.idle": "2024-12-21T08:45:22.521324Z",
     "shell.execute_reply": "2024-12-21T08:45:22.520102Z",
     "shell.execute_reply.started": "2024-12-21T08:45:10.867358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jiter, openai\n",
      "Successfully installed jiter-0.8.2 openai-1.58.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Round Dialogue Demonstration\n",
    "\n",
    "In this section, we will build a simple single-round dialogue through an API call. You can input a question, and the model will return a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T08:45:40.645955Z",
     "iopub.status.busy": "2024-12-21T08:45:40.645554Z",
     "iopub.status.idle": "2024-12-21T08:45:42.837464Z",
     "shell.execute_reply": "2024-12-21T08:45:42.836389Z",
     "shell.execute_reply.started": "2024-12-21T08:45:40.645916Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-d0ec996b-be56-949a-bca1-453d3a8e413f\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"我是来自阿里云的大规模语言模型，我叫通义千问。\",\"refusal\":null,\"role\":\"assistant\",\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1734770743,\"model\":\"qwen-turbo\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":16,\"prompt_tokens\":22,\"total_tokens\":38,\"completion_tokens_details\":null,\"prompt_tokens_details\":null}}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Define a function to get model responses\n",
    "def get_response():\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv('OPENAI_API_KEY'),  # Retrieve the API key from environment variables\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # Use Alibaba Cloud's large model API\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen-turbo\",\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "            {'role': 'user', 'content': '你是谁？'}\n",
    "        ]\n",
    "    )\n",
    "    print(completion.model_dump_json())\n",
    "\n",
    "# Call the function to engage in conversation\n",
    "get_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Round Dialogue Demonstration\n",
    "\n",
    "Extend the above code to support multi-round dialogues. This means the model can remember the context, thereby generating more coherent responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T08:47:11.737762Z",
     "iopub.status.busy": "2024-12-21T08:47:11.737387Z",
     "iopub.status.idle": "2024-12-21T08:47:51.312937Z",
     "shell.execute_reply": "2024-12-21T08:47:51.311853Z",
     "shell.execute_reply.started": "2024-12-21T08:47:11.737730Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入你的问题： 你是谁？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型回复：我是来自阿里云的大规模语言模型，我叫通义千问。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入你的问题： 你可以做什么？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型回复：我是一个能够回答问题、创作文字，还能表达观点、撰写代码的超大规模语言模型。如果您有任何问题或需要帮助，请随时告诉我，我会尽力提供支持。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入你的问题： 怎么对通义千问进行fine-tune？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型回复：对通义千问（Qwen）进行微调（Fine-tuning）通常需要访问特定的数据集和计算资源，并且需要遵循相关的使用政策和指南。作为普通用户，您可能无法直接对通义千问模型进行微调。不过，我可以告诉您一般微调过程中的几个关键步骤：\n",
      "\n",
      "1. **准备数据集**：首先，您需要准备一个高质量的数据集来微调模型。这个数据集应该包含与您希望模型学习的任务相关的输入输出对。\n",
      "\n",
      "2. **环境设置**：确保您的开发环境中安装了必要的库，例如TensorFlow或PyTorch等深度学习框架，以及阿里云提供的相关工具。\n",
      "\n",
      "3. **加载预训练模型**：使用阿里云提供的API或SDK加载预训练的通义千问模型作为基础模型。\n",
      "\n",
      "4. **数据处理**：将您的数据集转换为模型可以理解的格式，这通常涉及到分词、编码等预处理步骤。\n",
      "\n",
      "5. **微调模型**：使用准备好的数据集来微调模型。这一步骤会根据您的需求调整模型参数，以更好地适应特定任务。\n",
      "\n",
      "6. **评估和优化**：完成微调后，评估模型在验证集上的表现，并根据结果进一步调整模型参数或数据集，直到达到满意的性能。\n",
      "\n",
      "7. **部署模型**：最后，您可以将微调后的模型部署到生产环境中，以便进行实际应用。\n",
      "\n",
      "请注意，上述步骤是通用流程，具体操作可能会因阿里云的具体实现而有所不同。如果您有权限并且希望深入了解如何进行微调，建议查阅阿里云官方文档或联系技术支持获取更详细的指导。对于没有相关经验的用户来说，建议从简单的项目开始尝试，逐步积累经验。\n"
     ]
    }
   ],
   "source": [
    "def get_response(messages):\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv('OPENAI_API_KEY'), \n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen-turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "# Initialize the conversation history\n",
    "messages = [{'role': 'system', 'content': 'You are a helpful assistant.'}]\n",
    "\n",
    "# Engage in a multi-round dialogue\n",
    "for i in range(3):\n",
    "    user_input = input(\"请输入你的问题：\")\n",
    "    \n",
    "    # Add the user's question to the conversation history\n",
    "    messages.append({'role': 'user', 'content': user_input})\n",
    "    \n",
    "    # Get the model's response and print it\n",
    "    assistant_output = get_response(messages).choices[0].message.content\n",
    "    print(f'模型回复：{assistant_output}')\n",
    "    \n",
    "    # Add the model's response to the conversation history as well\n",
    "    messages.append({'role': 'assistant', 'content': assistant_output})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Output Demonstration\n",
    "\n",
    "Stream output allows us to view the model-generated responses in real-time, rather than waiting for the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T08:48:59.837326Z",
     "iopub.status.busy": "2024-12-21T08:48:59.836934Z",
     "iopub.status.idle": "2024-12-21T08:49:01.911295Z",
     "shell.execute_reply": "2024-12-21T08:49:01.910320Z",
     "shell.execute_reply.started": "2024-12-21T08:48:59.837291Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-f863ba65-284c-9b1f-bc0e-856f13722193\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":\"assistant\",\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1734770942,\"model\":\"qwen-turbo\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "{\"id\":\"chatcmpl-f863ba65-284c-9b1f-bc0e-856f13722193\",\"choices\":[{\"delta\":{\"content\":\"我是\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1734770942,\"model\":\"qwen-turbo\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "{\"id\":\"chatcmpl-f863ba65-284c-9b1f-bc0e-856f13722193\",\"choices\":[{\"delta\":{\"content\":\"阿里\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1734770942,\"model\":\"qwen-turbo\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "{\"id\":\"chatcmpl-f863ba65-284c-9b1f-bc0e-856f13722193\",\"choices\":[{\"delta\":{\"content\":\"云\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1734770942,\"model\":\"qwen-turbo\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "{\"id\":\"chatcmpl-f863ba65-284c-9b1f-bc0e-856f13722193\",\"choices\":[{\"delta\":{\"content\":\"开发的一款超大规模\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1734770942,\"model\":\"qwen-turbo\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "{\"id\":\"chatcmpl-f863ba65-284c-9b1f-bc0e-856f13722193\",\"choices\":[{\"delta\":{\"content\":\"语言模型，我\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1734770942,\"model\":\"qwen-turbo\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "{\"id\":\"chatcmpl-f863ba65-284c-9b1f-bc0e-856f13722193\",\"choices\":[{\"delta\":{\"content\":\"叫通义千\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1734770942,\"model\":\"qwen-turbo\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "{\"id\":\"chatcmpl-f863ba65-284c-9b1f-bc0e-856f13722193\",\"choices\":[{\"delta\":{\"content\":\"问。\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1734770942,\"model\":\"qwen-turbo\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "{\"id\":\"chatcmpl-f863ba65-284c-9b1f-bc0e-856f13722193\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null}],\"created\":1734770942,\"model\":\"qwen-turbo\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n",
      "{\"id\":\"chatcmpl-f863ba65-284c-9b1f-bc0e-856f13722193\",\"choices\":[],\"created\":1734770942,\"model\":\"qwen-turbo\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":17,\"prompt_tokens\":22,\"total_tokens\":39,\"completion_tokens_details\":null,\"prompt_tokens_details\":null}}\n"
     ]
    }
   ],
   "source": [
    "# Implement streaming output\n",
    "def get_response_stream():\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen-turbo\",\n",
    "        messages=[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "                  {'role': 'user', 'content': '你是谁？'}],\n",
    "        stream=True,\n",
    "        stream_options={\"include_usage\": True}\n",
    "    )\n",
    "    \n",
    "    # Output the generated results in real-time\n",
    "    for chunk in completion:\n",
    "        print(chunk.model_dump_json())\n",
    "\n",
    "# Call the streaming output function\n",
    "get_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
